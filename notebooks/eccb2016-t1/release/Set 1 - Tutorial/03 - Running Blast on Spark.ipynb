{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img style=\"float: left\" src=\"images/spark.png\" />\n",
    "<img style=\"float: right\" src=\"images/surfsara.png\" />\n",
    "<hr style=\"clear: both\" />\n",
    "\n",
    "# Running Blast with Apache Spark\n",
    "\n",
    "In our third notebook we will look at some metagenomics data. We will use blast(x) to compare our sequences to (part) of the NCBI non-redundant protein database.\n",
    "\n",
    "_You can edit the cells below and execute the code by selecting the cell and press Shift-Enter. Code completion is supported by use of the Tab key._\n",
    "\n",
    "During the exercises you may want to refer to the [PySpark documentation](https://spark.apache.org/docs/1.6.1/api/python/pyspark.html#pyspark.RDD) for more information on possible transformations and actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# initialize Spark\n",
    "from pyspark import SparkContext, SparkConf\n",
    "if not 'sc' in globals():\n",
    "    conf = SparkConf().setMaster('local[*]')\n",
    "    sc = SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RDD from a FASTA file\n",
    "\n",
    "We will start by creating an RDD from a FASTA file. We will read this using the `sc.textFile` method we also used in the previous notebooks. Note that this is a smaller sample with around a 100 sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pprint\n",
    "pp = pprint.PrettyPrinter(indent=2)\n",
    "\n",
    "reads = sc.textFile(\"file:////home/jovyan/work/data/blast/input/sample.fa\")\n",
    "\n",
    "pp.pprint(reads.take(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like before the division in records is incorrect. The metadata and sequence data of a single read are now split over two separate records. The clean solution to this would be to write a custom InputFormat in Java or Scala. For simplicity in this tutorial we will do some text munching in Spark to get the same effect.\n",
    "\n",
    "## Merge metadata and sequence into a single record\n",
    "\n",
    "In this case we are going to run NCBI BLAST - a third-party binary appplication - on the sequences and for that we will need to have the data in the exact format that BLAST expects (e.g. fasta metadata and sequence records). In order to create these records we can first add an index to all the records. We will then use a series of [`map`](https://spark.apache.org/docs/1.6.1/api/python/pyspark.html#pyspark.RDD.map) function applications to change the index of the odd-numberd records (subtract those indices by 1) and create tuples where the first element is the index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "indexedReads = reads.zipWithIndex()\n",
    "pp.pprint(indexedReads.take(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "59336b933aa9df7c8f2160745ad723dd",
     "grade": false,
     "grade_id": "e1",
     "locked": false,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# Write appropriate map functions to renumber odd indices and to switch the position of the tuple values.\n",
    "keyedReads = (indexedReads\n",
    "              # PLEASE FILL IN YOUR CODE HERE\n",
    "             )\n",
    "\n",
    "pp.pprint(keyedReads.take(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After executing the previous cell we should have an RDD that contains tuples with index, line-from-fasta pairs. The index of the records can be used to recreate the sequence and metadata combinations. In order to do so you can use the [`groupByKey`](https://spark.apache.org/docs/1.6.1/api/python/pyspark.html#pyspark.RDD.groupByKey) operation. This will, for each key create a list of values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "d6123eb043096d50de600df1700c4b3e",
     "grade": false,
     "grade_id": "e2",
     "locked": false,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# Create an RDD 'metaAndSeq' that is grouped by the index. \n",
    "# PLEASE FILL IN YOUR CODE HERE\n",
    "\n",
    "# Print the results (HINT: you can do a 'map' on the results of the grouping to cast \n",
    "# the iterable to a Python list: list(some_iterable)\n",
    "pp.pprint(metaAndSeq.take(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the grouping done, we can combine the values of the grouping into the metadata and sequence text combination. We will drop the index from the tuple in this step. The result should look like a single record (meta and sequence) from  fasta file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Merge the results of the grouping into a single string record.\n",
    "inputSeqs = metaAndSeq.map(lambda tup: tup[1][0] + \"\\n\" + tup[1][1])\n",
    "pp.pprint(inputSeqs.take(1)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Blasting the sequences\n",
    "With each record being a meta and sequence combination we can call blast to run on each of these records in parallel. The amount of parallel executions are determined here by the amount of RDD partitions. First print the amount of partitions of the inputSeq RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get the amount of partitions in the current RDD\n",
    "print inputSeqs.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As there are currently 12 cores assigned to this environment we can repartition the RDD in 12 parts so that 12 blast executions can be run in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Repartion the RDD; note that this means that data may be shuffled over the network\n",
    "inputSeqsRepart = inputSeqs.repartition(12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we can run blastx. We have created a small shell script on the host that we can call via the RDD [`pipe`](https://spark.apache.org/docs/1.6.1/api/python/pyspark.html#pyspark.RDD.pipe) function. This function pipes the contents of each record to a subprocess running the script or command you specified. Pipe will collect the standard output (each line as a record). \n",
    "\n",
    "While handy in this exercise - think of all the requirements needed on a typical cluster configurations and of all the possible things that could go wrong when running a workflow such as this at scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "1373c748c506789444646f7a2d08c53b",
     "grade": false,
     "grade_id": "e3",
     "locked": false,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# Run blastx for each of the records. Use the pipe function to run the command present in the script variable.\n",
    "# Name the resulting RDD 'blastedSeqs'\n",
    "script = \"/home/jovyan/work/data/blast/program/ncbi-blast-2.4.0+/runblast\"\n",
    "# PLEASE FILL IN YOUR CODE HERE\n",
    "pp.pprint(blastedSeqs.take(4))\n",
    "blastedSeqs.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Post-processing 1: taxonomic distribution of Blast hits\n",
    "\n",
    "Spark, Pyspark and Jupyter together provide a great platform for interactive analysis. It is possible to have different views on your data, work at scale and leverage existing Python and Scala libraries. In this next section we will use the [ETE toolkit](http://etetoolkit.org) to enrich our blast data with extra taxonomic information and look at the distribution of organisms inferred from the blast hits at various taxonomic ranks.\n",
    "\n",
    "Before enriching we will restructure the blast output. First split all the output lines on the tab character and create a blast report with only the read id, blast hit, e-value, tax id and species elements. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Split the blast output lines\n",
    "blastedSeqsVals = blastedSeqs.map(lambda s: s.split(\"\\t\"))\n",
    "\n",
    "# Create a report with only a subset of the data. Some fields contain multiple matches. For demonstration\n",
    "# purposes we only consider the first of such a value. What would be a better way to deal with this?\n",
    "annotatedReport = blastedSeqsVals.map(lambda a: [a[0], a[1], a[5], a[8].split(\";\")[0], a[9].split(\";\")[0]])\n",
    "print annotatedReport.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With that in place we can start add other data to our annotation. We want to have an idea of the makeup of our sample at different taxonomic ranks. The ETE toolkit has functionality for parsing the NCBI taxonomy data files. We will start by defining a function that from a tax id present in the blast output can infer the taxonomic hierarchy of each tax id (the lineage). We will only extract the kingdom, phylum and class for a given tax id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from ete3 import NCBITaxa\n",
    "def ncbiQueryLineage(taxid):\n",
    "    try:\n",
    "        ncbi = NCBITaxa()\n",
    "        lineage = ncbi.get_lineage(taxid)\n",
    "        names = ncbi.get_taxid_translator(lineage)\n",
    "        report = [names[taxid] for taxid in lineage]\n",
    "        return [report[2], report[3], report[4]]\n",
    "    except ValueError as e: \n",
    "        return [\"\", \"\", \"\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets test our function on two id's. The first time `NCBITaxa` is run the database is populated. This may take some time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pp.pprint(ncbiQueryLineage(525371))\n",
    "pp.pprint(ncbiQueryLineage(696844))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now use a `map` to apply the function to the tax id in our annotatedReport"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "taxReport = annotatedReport.map(lambda a: a + ncbiQueryLineage(a[3]))\n",
    "taxReport.cache()\n",
    "pp.pprint(taxReport.take(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Post-processing 2: Plotting taxonomic distributions\n",
    "\n",
    "With the enrichment in place we can make a flew plots of the frequencies of distinct species, phyla and kingdoms. In this exercise we will also convert our data to a [`DataFrame`](http://spark.apache.org/docs/1.6.1/api/python/pyspark.sql.html#pyspark.sql.DataFrame). DataFrames are great for working with tabular, typed data and support a custom DSL for data selection as well as the possibility to run SQL queries against the data stored in the DataFrame.\n",
    "\n",
    "First we will define the structure of our table by defining a [`Row`](https://spark.apache.org/docs/1.6.1/api/python/pyspark.sql.html#pyspark.sql.Row) object to use when converting records in the RDD to Rows in the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "taxRecord = Row(\"readid\", \"giid\", \"evalue\", \"taxid\", \"species\", \"kingdom\", \"phylum\", \"class\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The values in our RDD are all strings. Cast the e-value to a float before converting to a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "taxReportRows = taxReport.map(lambda a: taxRecord(a[0], a[1], float(a[2]), a[3], a[4], a[5], a[6], a[7]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we can initialize a [`SQLContext`](https://spark.apache.org/docs/1.6.1/api/python/pyspark.sql.html#pyspark.sql.SQLContext) to convert our RDD to a DataFrame. Do the conversion, print the schema and the first 10 records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "\n",
    "sqlCtx = SQLContext(sc)\n",
    "taxReportDF = sqlCtx.createDataFrame(taxReportRows)\n",
    "taxReportDF.printSchema()\n",
    "taxReportDF.select(\"readid\", \"giid\", \"species\", \"phylum\").show(n=10, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can use functions on this DataFrame to group on columns, count values, sort etc. If you have worked with relational databases this will look very familiar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "speciesDistribution = taxReportDF.select(\"readid\", \"species\").groupBy(\"species\").count().sort(col(\"count\").desc())\n",
    "speciesDistribution.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jupyter integrates nicely with plotting libraries such as [matplotlib](http://matplotlib.org). Create a simple bargraph of the species that where found more than 15 times in the blast report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "speciesPlotVals =  speciesDistribution.filter(col(\"count\") >= 15).rdd.map(lambda r: (r['species'], r['count']))\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcdefaults()\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "species = speciesPlotVals.keys().collect()\n",
    "y_pos = np.arange(len(species))\n",
    "freq = speciesPlotVals.values().collect()\n",
    "\n",
    "plt.barh(y_pos, freq, align='center')\n",
    "plt.yticks(y_pos, species)\n",
    "plt.xlabel('Frequency')\n",
    "plt.title('Distribution of species matches in sample')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can do the same for the phylum and kingdom level in the taxonomic hierarchy. Fill in the relevant sections to create these graphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "35e6b2d6a3b1d3c4caa9991d9641127b",
     "grade": false,
     "grade_id": "e4",
     "locked": false,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# PLEASE FILL IN YOUR CODE HERE\n",
    "\n",
    "phylum = phylumPlotVals.keys().collect()\n",
    "y_pos = np.arange(len(phylum))\n",
    "freq = phylumPlotVals.values().collect()\n",
    "\n",
    "plt.barh(y_pos, freq, align='center')\n",
    "plt.yticks(y_pos, phylum)\n",
    "plt.xlabel('Frequency')\n",
    "plt.title('Distribution of phylum matches in sample')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "59987f59c88f05fb8d8953c1264f5658",
     "grade": false,
     "grade_id": "e5",
     "locked": false,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# PLEASE FILL IN YOUR CODE HERE\n",
    "\n",
    "kingdom = kingDomPlotVals.keys().collect()\n",
    "y_pos = np.arange(len(kingdom))\n",
    "freq = kingDomPlotVals.values().collect()\n",
    "\n",
    "plt.barh(y_pos, freq, align='center')\n",
    "plt.yticks(y_pos, kingdom)\n",
    "plt.xlabel('Frequency')\n",
    "plt.title('Distribution of kingdom matches in sample')\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
