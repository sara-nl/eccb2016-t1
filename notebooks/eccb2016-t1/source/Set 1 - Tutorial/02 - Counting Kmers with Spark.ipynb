{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img style=\"float: left\" src=\"images/spark.png\" />\n",
    "<img style=\"float: right\" src=\"images/surfsara.png\" />\n",
    "<hr style=\"clear: both\" />\n",
    "\n",
    "# Counting kmers with Apache Spark\n",
    "\n",
    "In our second notebook we will look at genomics data and count the numer of occurences of specific subsequences in the reads.\n",
    "\n",
    "_You can edit the cells below and execute the code by selecting the cell and press Shift-Enter. Code completion is supported by use of the Tab key._\n",
    "\n",
    "During the exercises you may want to refer to the [PySpark documentation](https://spark.apache.org/docs/1.6.1/api/python/pyspark.html#pyspark.RDD) for more information on possible transformations and actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# initialize Spark\n",
    "from pyspark import SparkContext, SparkConf\n",
    "\n",
    "if not 'sc' in globals(): # This Python 'trick' makes sure the SparkContext sc is initialized exactly once\n",
    "    conf = SparkConf().setMaster('local[*]')\n",
    "    sc = SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  RDD from a FASTA file\n",
    "\n",
    "We will start by creating an RDD from a FASTA file. We will read this using the `sc.textFile` method we also used in the first notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pprint\n",
    "pp = pprint.PrettyPrinter(indent=2)\n",
    "\n",
    "reads = sc.textFile(\"file:////home/jovyan/work/data/blast/input/CAM_SMPL_GS108.fa\")\n",
    "\n",
    "pp.pprint(reads.take(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that we are able to read the FASTA file, but the division in records is incorrect. The metadata and sequence data of a single read are now split over two separate records. The clean solution to this would be to write a custom InputFormat in Java or Scala. For simplicity in this tutorial we will do some text munching in Spark to get the same effect.\n",
    "\n",
    "## Remove metadata\n",
    "\n",
    "In this case we are not interested in the metadata. We select only the sequence data by adding an index number to all the records and use a [`filter`](https://spark.apache.org/docs/1.6.1/api/python/pyspark.html#pyspark.RDD.filter) to select only the odd-numberd records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "indexed_reads = reads.zipWithIndex()\n",
    "\n",
    "pp.pprint(indexed_reads.take(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create an RDD 'indexed_sequences' from' indexed_reads' that only contains the records with an odd index number.\n",
    "\n",
    "### BEGIN SOLUTION\n",
    "indexed_sequences = indexed_reads.filter(lambda x: x[1] % 2 == 1)\n",
    "### END SOLUTION\n",
    "\n",
    "sequences = indexed_sequences.keys().cache()\n",
    "\n",
    "pp.pprint(sequences.take(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examining the data\n",
    "\n",
    "Before we get started, let's have a quick look at the data. How many sequences are there in total in this file?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### BEGIN SOLUTION\n",
    "num_seq = sequences.count()\n",
    "### END SOLUTION\n",
    "\n",
    "print(num_seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the length of the shorted and longest sequence in this file? [`RDD`](https://spark.apache.org/docs/1.6.1/api/python/pyspark.html#pyspark.RDD) has two useful methods for this.\n",
    "\n",
    "Extra: Also determine the average length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "seq_lengths = sequences.map(len).cache()\n",
    "\n",
    "### BEGIN SOLUTION\n",
    "shortest = seq_lengths.min()\n",
    "longest = seq_lengths.max()\n",
    "### END SOLUTION\n",
    "\n",
    "print('The shortest sequence has length: ' + str(shortest))\n",
    "print('The longest sequence has length: ' + str(longest))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extending WordCount: BaseCount\n",
    "\n",
    "Now that we have our input data in the format we want we can start to do something with it. Our first exercise is a variation on the WordCount we did in the first notebook. This time we will not count words, but the individual bases in the sequences.\n",
    "\n",
    "Hint: the easiest way in Python to split a string `s` into characters is: `list(s)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create an RDD 'bases' from 'sequences' containing all the individual bases (letters).\n",
    "\n",
    "### BEGIN SOLUTION\n",
    "bases = sequences.flatMap(lambda s: list(s))\n",
    "### END SOLUTION \n",
    "\n",
    "pp.pprint(bases.take(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from test_helper import Test\n",
    "\n",
    "Test.assertEquals(bases.take(5), [u'A', u'T', u'T', u'T', u'A'], 'incorrect value for bases')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have all the bases, all we need to do is count them by creating (key, 1) tuples and sum these per key. We can chain these two operations on a single line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create an RDD 'basecount' from 'bases' with (base, #occurences) pairs.\n",
    "\n",
    "### BEGIN SOLUTION\n",
    "basecounts = bases.map(lambda b: (b, 1)).reduceByKey(lambda a,b: a + b)\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we know the number of records in this RDD is very small (5), it is safe to call `collect` on the RDD and print the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pp.pprint(basecounts.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Test.assertEquals(sorted(basecounts.collect()),\n",
    "                  [(u'A', 189420125), (u'C', 89225753), (u'G', 90183181), (u'N', 134734), (u'T', 187685193)],\n",
    "                  'incorrect value for basecounts')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extending BaseCount: KmerCount\n",
    "\n",
    "We have counted all the 1-mers. Let's extend this to the general case of k-mers. For this we need to generate all overlapping substrings of length k. We can no longer just split the sequence, but need to use a sliding window. We have already written a helper function for you that does this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sliding(seq, size):\n",
    "    result = []\n",
    "    for i in range(0, len(seq) - size + 1):\n",
    "        result.append(seq[i:i + size])\n",
    "    return result\n",
    "\n",
    "print(sliding(\"GAGATCTCCTGTGGTGTCCTTGGTCATAGTGATTTGCTCCTACAA\", 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an RDD with all the subsequences of length 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create an RDD 'kmers' from 'sequences' with all the 10-mers\n",
    "\n",
    "### BEGIN SOLUTION\n",
    "kmers = sequences.flatMap(lambda s: sliding(s, 10))\n",
    "### END SOLUTION\n",
    "\n",
    "print(kmers.take(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Test.assertEquals(kmers.take(5),\n",
    "                  [u'ATTTACAATA', u'TTTACAATAA', u'TTACAATAAT', u'TACAATAATT', u'ACAATAATTT'],\n",
    "                  'incorrect value for kmers')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count all the unique values in the RDD. Looks familiar?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create an RDD 'kmercounts' from 'kmers' with (10-mer, #occurences) pairs.\n",
    "\n",
    "### BEGIN SOLUTION\n",
    "kmercounts = kmers.map(lambda x: (x, 1)).reduceByKey(lambda x,y: x + y)\n",
    "### END SOLUTION\n",
    "\n",
    "kmercounts.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use `takeOrdered` to get the 10 most frequent 10-mers. This may take some time (~ 2 minutes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### BEGIN SOLUTION\n",
    "top10_kmers = kmercounts.takeOrdered(10, key=lambda x: -x[1])\n",
    "### END SOLUTION\n",
    "\n",
    "print(top10_kmers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Test.assertEquals(top10_kmers,\n",
    "                  [(u'AAAAATAAAA', 53752), (u'AAAAAAATAA', 50949), (u'AAAAAATAAA', 50909), (u'AAAAAATTAA', 48357),\n",
    "                   (u'AAAAAAATTA', 48239), (u'TTTTATTTTT', 47295), (u'AAAAATTAAA', 46774), (u'AAAATAAAAA', 45347),\n",
    "                   (u'AAAATTAAAA', 44951), (u'TTTTTAAAAA', 43321)], \n",
    "                  'incorrect value for top10_kmers')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we look at the distribution of 10-mers. We want to know how many unique 10-mers occur only once, how many twice, etc. We will plot the results using Python's matplotlib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create an RDD 'kmerdist' from 'kmercounts' with (#occurences, #unique kmers)\n",
    "\n",
    "### BEGIN SOLUTION\n",
    "kmerdist = kmercounts.map(lambda x: (x[1], 1)).reduceByKey(lambda x, y: x + y)\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# For the plot we sort the RDD by numer of unique kmers\n",
    "kmerdistsorted = kmerdist.map(lambda x: (x[1], x[0])).sortByKey(True).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x = kmerdistsorted.keys().collect()\n",
    "y = kmerdistsorted.values().collect()\n",
    "\n",
    "plt.plot(x, y)\n",
    "plt.yscale('log')\n",
    "plt.title(\"kmer distribution\")\n",
    "plt.xlabel(\"kmer matches\")\n",
    "plt.ylabel(\"genome-wide frequency\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional 1: [1,k]-mers in a single go\n",
    "\n",
    "Create a version of the KmerCount example that counts all subsequenecs of length 1,2,3,...,k. Don't use any `for` loops over RDDs (on a single record is OK). And don't use `collect` to combine results.\n",
    "\n",
    "## Optional 2: Runtime as a function of k\n",
    "\n",
    "You might have noticed that the runtime increases when you increase the number k. Look at the runtime of the KmerCount for different values of k and plot these in a graph. You can get an estimate of the runtime of a notebook cell by putting `%time` on the first line.\n",
    "\n",
    "## End of the second notebook\n",
    "\n",
    "Congratulations! You finished the second notebook. You can continue to the next notebook \"03 - Running Blast on Spark\". Close this notebook via 'File' -> 'Close and Halt' to stop the underlying kernel and release computing resources."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
